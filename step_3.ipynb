{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import scipy.fftpack as fftpack\n",
    "import scipy.signal as signal\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(audio_file_path):\n",
    "    audio, sr = librosa.load(audio_file_path, sr=44100)\n",
    "    return audio, sr\n",
    "\n",
    "def load_model(model_path):\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(audio, sr):\n",
    "    \"\"\"\n",
    "    Extracts multiple audio features (MFCC, Chroma STFT, Chroma CQT, Chroma CENS) from the audio signal.\n",
    "    Returns a 3D vector (time, frequency, feature).\n",
    "    \"\"\"\n",
    "    # Feature 1: MFCC\n",
    "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "    mfcc_mean = np.mean(mfcc, axis=1)\n",
    "\n",
    "    # Feature 2: Chroma STFT\n",
    "    chroma_stft = librosa.feature.chroma_stft(y=audio, sr=sr)\n",
    "    chroma_stft_mean = np.mean(chroma_stft, axis=1)\n",
    "\n",
    "    # Feature 3: Chroma CQT with reduced bins\n",
    "    chroma_cqt = librosa.feature.chroma_cqt(y=audio, sr=sr, bins_per_octave=24, n_octaves=7)\n",
    "    chroma_cqt_mean = np.mean(chroma_cqt, axis=1)\n",
    "\n",
    "    # Feature 4: Chroma CENS\n",
    "    chroma_cens = librosa.feature.chroma_cens(y=audio, sr=sr)\n",
    "    chroma_cens_mean = np.mean(chroma_cens, axis=1)\n",
    "\n",
    "    # Combine all features into a single vector\n",
    "    return np.hstack([mfcc_mean, chroma_stft_mean, chroma_cqt_mean, chroma_cens_mean])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.5140222e+02,  1.9458733e+02,  3.2716966e-01,  2.0734904e+00,\n",
       "        1.5886181e+01,  7.7973294e+00, -6.3988641e-02,  7.4882021e+00,\n",
       "        3.2788143e+00,  2.5555868e+00,  1.5345799e+00,  5.0446253e+00,\n",
       "       -8.0071753e-01,  6.5317470e-01,  6.3637036e-01,  6.8927950e-01,\n",
       "        6.6140944e-01,  6.6034132e-01,  6.9679976e-01,  6.7176622e-01,\n",
       "        6.5601957e-01,  6.7960477e-01,  7.3309726e-01,  7.1521008e-01,\n",
       "        6.8094528e-01,  6.2673324e-01,  9.1154474e-01,  6.4522040e-01,\n",
       "        6.7497098e-01,  6.3973063e-01,  6.3435423e-01,  5.1672471e-01,\n",
       "        5.8784765e-01,  6.1207128e-01,  5.2384591e-01,  5.3879994e-01,\n",
       "        5.1614672e-01,  3.0679300e-01,  4.1880953e-01,  2.9451969e-01,\n",
       "        2.7207908e-01,  2.9391468e-01,  2.6854828e-01,  2.0644854e-01,\n",
       "        2.9108116e-01,  2.6673645e-01,  2.2177199e-01,  2.4278879e-01,\n",
       "        2.3804136e-01], dtype=float32)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_file_path = \"178686-0-0-34.wav\"\n",
    "audio,sr=load_audio(audio_file_path)\n",
    "features = extract_features(audio,sr)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=features.reshape(-1,1)\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_features = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.99999994],\n",
       "       [0.69928277],\n",
       "       [0.7019861 ],\n",
       "       [0.7233683 ],\n",
       "       [0.71084666],\n",
       "       [0.69867724],\n",
       "       [0.71036816],\n",
       "       [0.70385194],\n",
       "       [0.7027324 ],\n",
       "       [0.70115185],\n",
       "       [0.70658547],\n",
       "       [0.69753677],\n",
       "       [0.69978744],\n",
       "       [0.6997614 ],\n",
       "       [0.69984335],\n",
       "       [0.6998002 ],\n",
       "       [0.6997985 ],\n",
       "       [0.69985497],\n",
       "       [0.6998162 ],\n",
       "       [0.69979185],\n",
       "       [0.6998283 ],\n",
       "       [0.6999112 ],\n",
       "       [0.69988346],\n",
       "       [0.6998304 ],\n",
       "       [0.6997465 ],\n",
       "       [0.7001874 ],\n",
       "       [0.6997751 ],\n",
       "       [0.6998212 ],\n",
       "       [0.69976664],\n",
       "       [0.6997583 ],\n",
       "       [0.6995762 ],\n",
       "       [0.6996863 ],\n",
       "       [0.6997238 ],\n",
       "       [0.6995872 ],\n",
       "       [0.69961035],\n",
       "       [0.6995753 ],\n",
       "       [0.69925123],\n",
       "       [0.6994246 ],\n",
       "       [0.6992322 ],\n",
       "       [0.6991975 ],\n",
       "       [0.69923127],\n",
       "       [0.69919205],\n",
       "       [0.6990959 ],\n",
       "       [0.6992269 ],\n",
       "       [0.69918925],\n",
       "       [0.6991196 ],\n",
       "       [0.6991522 ],\n",
       "       [0.6991448 ]], dtype=float32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_features = scaled_features.reshape(-1, 1, 49, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"trained_cnn_model.h5\")\n",
    "predictions = model.predict(scaled_features)\n",
    "print(np.argmax(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Noise Type: children_playing\n",
      "Selected Threshold for children_playing: 0.15\n",
      "Denoised audio saved as 'denoised_output.wav'.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Map prediction index to noise type\n",
    "noise_classes = [\n",
    "    \"air_conditioner\", \"car_horn\", \"children_playing\", \"dog_bark\",\n",
    "    \"drilling\", \"engine_idling\", \"gun_shot\", \"jackhammer\",\n",
    "    \"siren\", \"street_music\"\n",
    "]\n",
    "\n",
    "predicted_index = np.argmax(predictions)\n",
    "predicted_noise_type = noise_classes[predicted_index]\n",
    "print(f\"Predicted Noise Type: {predicted_noise_type}\")\n",
    "\n",
    "# Step 2: Select threshold based on predicted noise type\n",
    "noise_thresholds = {\n",
    "    \"air_conditioner\": 0.05,\n",
    "    \"car_horn\": 0.1,\n",
    "    \"children_playing\": 0.15,\n",
    "    \"dog_bark\": 0.2,\n",
    "    \"drilling\": 0.25,\n",
    "    \"engine_idling\": 0.1,\n",
    "    \"gun_shot\": 0.3,\n",
    "    \"jackhammer\": 0.35,\n",
    "    \"siren\": 0.2,\n",
    "    \"street_music\": 0.15\n",
    "}\n",
    "\n",
    "threshold = noise_thresholds[predicted_noise_type]\n",
    "print(f\"Selected Threshold for {predicted_noise_type}: {threshold}\")\n",
    "\n",
    "# Step 3: Compute FFT and create noise mask\n",
    "signal_fft = fftpack.fft(audio)\n",
    "signal_power = np.abs(signal_fft) ** 2\n",
    "\n",
    "# Generate mask\n",
    "mask = signal_power > threshold\n",
    "mask = mask.astype(float)\n",
    "\n",
    "# Apply mask to reduce noise\n",
    "reduced_fft = signal_fft * mask\n",
    "denoised_audio = np.real(fftpack.ifft(reduced_fft))\n",
    "\n",
    "# Step 4: Save the denoised audio\n",
    "sf.write(\"denoised_output.wav\", denoised_audio, sr)\n",
    "print(\"Denoised audio saved as 'denoised_output.wav'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
